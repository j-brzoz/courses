{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3e1e02",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search + AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4c1d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d3aa8",
   "metadata": {},
   "source": [
    "##### Game of tic tac toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d8358ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count # all possible actions\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'TicTacToe'\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count # win in a row\n",
    "            or np.sum(state[:, column]) == player * self.row_count # win in a coulmn\n",
    "            or np.sum(np.diag(state)) == player * self.row_count # win in the first diagonal\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count # win in the second diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True # a win\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True # a draw\n",
    "        return 0, False # other\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state): # for the model\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d4860",
   "metadata": {},
   "source": [
    "##### human vs human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2434776",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "1:0\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "-1:1\n",
      "[[ 1. -1.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [2, 3, 4, 5, 6, 7, 8]\n",
      "1:2\n",
      "[[ 1. -1.  1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [3, 4, 5, 6, 7, 8]\n",
      "-1:3\n",
      "[[ 1. -1.  1.]\n",
      " [-1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [4, 5, 6, 7, 8]\n",
      "1:4\n",
      "[[ 1. -1.  1.]\n",
      " [-1.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [5, 6, 7, 8]\n",
      "-1:1\n",
      "action not valid\n",
      "[[ 1. -1.  1.]\n",
      " [-1.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [5, 6, 7, 8]\n",
      "-1:6\n",
      "[[ 1. -1.  1.]\n",
      " [-1.  1.  0.]\n",
      " [-1.  0.  0.]]\n",
      "valid_moves [5, 7, 8]\n",
      "1:8\n",
      "[[ 1. -1.  1.]\n",
      " [-1.  1.  0.]\n",
      " [-1.  0.  1.]]\n",
      "1 won\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    valid_moves = tictactoe.get_valid_moves(state)\n",
    "    print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "    action = int(input(f'{player}:'))\n",
    "    \n",
    "    if valid_moves[action] == 0:\n",
    "        print('action not valid')\n",
    "        continue\n",
    "    \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('draw')\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f052fd9",
   "metadata": {},
   "source": [
    "##### Node for MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        \n",
    "        self.children = []\n",
    "        self.expandable_moves = game.get_valid_moves(state)\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "    \n",
    "    def select(self): # get the most promising child\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 # value between 0 and 1\n",
    "        return q_value + self.args['C'] * np.sqrt(np.log(self.visit_count) / child.visit_count)\n",
    "    \n",
    "    def expand(self):\n",
    "        # pick a random legal action\n",
    "        action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
    "        self.expandable_moves[action] = 0\n",
    "        \n",
    "        # get a child state given an action\n",
    "        child_state = self.state.copy()\n",
    "        child_state = self.game.get_next_state(child_state, action, 1)\n",
    "        child_state = self.game.change_perspective(child_state, player=-1) # we need to switch the perspective\n",
    "        \n",
    "        # create the node\n",
    "        child = Node(self.game, self.args, child_state, self, action)\n",
    "        # add to the children\n",
    "        self.children.append(child)\n",
    "        \n",
    "        return child\n",
    "    \n",
    "    def simulate(self):\n",
    "        # check if we are in the terminal state\n",
    "        value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        \n",
    "        # if we are in the terminal state return expected value\n",
    "        if is_terminal:\n",
    "            return value\n",
    "        \n",
    "        # simulate until we reach terminal state\n",
    "        rollout_state = self.state.copy()\n",
    "        rollout_player = 1\n",
    "        while True:\n",
    "            # pick a random legal action\n",
    "            valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "            action = np.random.choice(np.where(valid_moves == 1)[0])\n",
    "            \n",
    "            # get a child state given an action\n",
    "            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "            \n",
    "            # check if we are in the terminal state\n",
    "            value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "            \n",
    "            # if we are in the terminal state return expected value\n",
    "            if is_terminal:\n",
    "                if player == -1:\n",
    "                    value = self.game.get_opponent_value(value) # we need to switch the perspective if opponent won\n",
    "                return value\n",
    "            \n",
    "            rollout_player = self.game.get_opponent(rollout_player)\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        # update values\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        # flip the value for the parent\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        \n",
    "        # update the parent\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db84c4",
   "metadata": {},
   "source": [
    "##### Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd337b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, game, args):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        \n",
    "    def search(self, state):\n",
    "        # make a root\n",
    "        root = Node(self.game, self.args, state)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # find the best not expanded node\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check if the node is terminal\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            # if not terminal, expand the node + simulate the resut\n",
    "            if not is_terminal:\n",
    "                node = node.expand()\n",
    "                value = node.simulate()\n",
    "            \n",
    "            # update the values\n",
    "            node.backpropagate(value)\n",
    "            \n",
    "        \n",
    "        # calculate probabilities for children\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ea9fe",
   "metadata": {},
   "source": [
    "##### ResBlock for ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a062bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91720ac3",
   "metadata": {},
   "source": [
    "##### ResNet for AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2130aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden):\n",
    "        super().__init__()\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size),\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backbone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c02e96",
   "metadata": {},
   "source": [
    "##### Node for AlphaMCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bfa23301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaNode:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self): # get the most promising child\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 # value between 0 and 1\n",
    "        return q_value + self.args['C'] * (np.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior # include prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        # expand all children at the same time\n",
    "        for action, prob in enumerate(policy):\n",
    "            # expand only legal moves\n",
    "            if prob > 0:\n",
    "                 # get a child state given an action\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                # create the node\n",
    "                child = AlphaNode(self.game, self.args, child_state, self, action, prob)\n",
    "                # add to the children\n",
    "                self.children.append(child) \n",
    "    \n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        # update values\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        # flip the value for the parent\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        \n",
    "        # update the parent\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208912f5",
   "metadata": {},
   "source": [
    "##### AlphaMCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64baa028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaMCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        # make a root\n",
    "        root = AlphaNode(self.game, self.args, state)\n",
    "        \n",
    "        # get the policy from the model accoriding to the state\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state)).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # apply softmax\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # add noise\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        # remove illegal moves\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        \n",
    "        # make sure probabilities add up to 1\n",
    "        policy /= np.sum(policy)\n",
    "        \n",
    "        # expand the root with policy\n",
    "        root.expand(policy)\n",
    "        \n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # find the best not expanded node\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check if the node is terminal\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            # if not terminal, expand the node\n",
    "            if not is_terminal:\n",
    "                # get policy and value from the model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                )\n",
    "                \n",
    "                # apply softmax on policy\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                \n",
    "                # remove illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                \n",
    "                # make sure probabilities add up to 1\n",
    "                policy /= np.sum(policy)\n",
    "            \n",
    "                # get numerical value\n",
    "                value = value.item()\n",
    "                \n",
    "                # expand the node with policy\n",
    "                node.expand(policy)\n",
    "            \n",
    "            # update the values\n",
    "            node.backpropagate(value)\n",
    "            \n",
    "        \n",
    "        # calculate probabilities for children\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a786e",
   "metadata": {},
   "source": [
    "##### AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "209599c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.aplhaMCTS = AlphaMCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            # get neutral state\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            \n",
    "            # get actions distribution form AlphaMCTS\n",
    "            action_probs = self.aplhaMCTS.search(neutral_state)\n",
    "            \n",
    "            # add state, probabilities and player to the memory \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            # perform an random weighted action from actions distribution\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            \n",
    "            # get the state after the action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            # check if the state is terminal\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                # what will be returned\n",
    "                returnMemory = []\n",
    "                \n",
    "                # for every example in memory\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    # calculate the final outcome \n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    \n",
    "                    # add encoded state, action probabilities and final outcome to the returned memory\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            # if not terminal, change the player\n",
    "            player = self.game.get_opponent(player)\n",
    "    \n",
    "    def train(self, memory):\n",
    "        # shuffle memory\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        # for every batch\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            \n",
    "            # get the batch\n",
    "            batch = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            \n",
    "            # get the state and approppriate targets\n",
    "            state, policy_targets, value_targets = zip(*batch)\n",
    "            \n",
    "            # convert to torch tensors\n",
    "            state = torch.tensor(np.array(state), dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(np.array(policy_targets), dtype=torch.float32)\n",
    "            value_targets = torch.tensor(np.array(value_targets).reshape(-1, 1), dtype=torch.float32)\n",
    "            \n",
    "            # get results from the model\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            # calulate the loss between expected targets and results from the model\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # backpropagation\n",
    "            self.optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            # where we will store games from self play\n",
    "            memory = []\n",
    "            \n",
    "            # switch model to eval mode\n",
    "            self.model.eval()\n",
    "            # self play to generate data\n",
    "            for selfPlayIteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            # switch model to train mode\n",
    "            self.model.train()\n",
    "            # train model with data from self play\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            # save model and optimizer after each iteration\n",
    "            torch.save(self.model.state_dict(), f'model_{iteration}_{self.game}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'optimizer_{iteration}_{self.game}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b017f6e",
   "metadata": {},
   "source": [
    "##### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee9f7a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1993e413b17f41f69bca66801c38f026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd9e324850f48c78be65ee2df4d7b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e1e9820b1f428781ebed7b62ea07d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85da16382d294f05a600cd254084ce78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828f1e0949754f4eb204d6beb041a7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6a7805826a410fa0ca24a4aba0e498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 32,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 32,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea44d91",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffc8b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6866602301597595\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.load_state_dict(torch.load('model_2_TicTacToe.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6447009e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "1:4\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [-1.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 5, 7, 8]\n",
      "1:1\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [-1.  0.  0.]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [-1. -1.  0.]]\n",
      "valid_moves [0, 2, 3, 5, 8]\n",
      "1:8\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [-1. -1.  1.]]\n",
      "[[-1.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [-1. -1.  1.]]\n",
      "valid_moves [2, 3, 5]\n",
      "1:3\n",
      "[[-1.  1.  0.]\n",
      " [ 1.  1.  0.]\n",
      " [-1. -1.  1.]]\n",
      "[[-1.  1.  0.]\n",
      " [ 1.  1. -1.]\n",
      " [-1. -1.  1.]]\n",
      "valid_moves [2]\n",
      "1:2\n",
      "[[-1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [-1. -1.  1.]]\n",
      "draw\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 32,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 32,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.load_state_dict(torch.load('model_2_TicTacToe.pt'))\n",
    "model.eval()\n",
    "\n",
    "mcts = AlphaMCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtcs-alphazero",
   "language": "python",
   "name": "mtcs-alphazero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
